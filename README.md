We conducted a detailed spatiotemporal epidemiologic analysis that examines the social, economic, and demographic associations with COVID-19 in Orange County (OC) from January 22, 2020 - January 25, 2021. We are interested in the heterogeneity in risks for test positivity and death within Orange County. We used a combination of datasets that include: individual records of PCR test results, individual records of deaths, and ZIP code-level sociodemographic data. Our main results have not yet been determined.
COVID-19 has appeared in different ways across social, economic, and demographic groups, and this is especially true for the diverse and high-density population of OC. Certain communities are more privileged and face less challenges in education, household income, access to healthcare, and life expectancy - all of which b by social, economic, demographic, and even political factors. Identifying predictors that cause test positivity and mortality disparities can give us insight into how we can better address the needs of diverse populations and mitigate risk.

Our experiments were made by differing the undersampling coefficient ranging from (0.013 - 1) which is equivalent to totally undersampling and no oversampling totally oversampling and no undersampling. Clearly after undersampling the majority class, the minority class is oversampled upto the new size of majority class therefore low values for the coefficient are equivalent to low training set size.
For each coeff value we used three classifiers: 1- XGBOOST 2- Logistic Regression 3- Random Forest. (Since the problem is actually a binary classification problem we used classifiers to solve it). For each experiment we recorded the four common performance measures along with the time elapsed for training and predicting and confusion matrix.


In order to conduct a statistical analysis of the coronavirus disease, we utilized two separate datasets reflecting the mortality and positivity of COVID-19 data in Orange County, California. Both datasets came from the Orange County Health Care Agency (OCHCA) and includes test cases from 01/22/2020 - 01/25/2021. We selected these dates to include both the first and second wave of the pandemic in hopes of capturing the different variables that factored into the spread of COVID-19. In order to consider other possible population density variables, zip code data (figure 3) that included features like percentage of adults with a bachelorâ€™s degree or insurance, median household income, and even area of the region, was merged with both the positivity and mortality dataset. This data came from the 2018 American Community Survey.
Figure one illustrates the dataset on test positivity which contains all of the PCR test data for patients in Orange County. This dataset included patient information like age, sex, race, ethnicity, and zip code. Additionally each patient had a different description of the PCR test described by: test_result and covid_positive, for the results of the test; posted_date, reflecting the date patients took the test; and time_days, is the number of days since the start date in January. Originally, the dataset on test positivity had 2,605,761 cases with 12 different features. After cleaning and filtering the data to fit our time frame and include additional zip code data, the test positivity had only 1,865,151 entries but with 27 total features. It is also important to note that for those patients that had multiple positive PCR tests, only the first positive test was recorded in the data.

The next dataset focused on mortality cases among those who have already tested positive for COVID-19, shown in figure two. This dataset contains 211,251 death cases in the twelve month period. However, out of the 211,251 cases only 2,671 cases were attributed to COVID-19. This test mortality dataset had 15 total features including reported city, death date, gender, sex, and zip code information.
Compared to our positivity data, this dataset contains a column named, death_due_to_covid, which is a binary response to whether or not the patient died due to COVID-19.

Accuracy: The percent of data which are correctly classified. (poor measure for imbalanced data) Precision: The percent of correct predictions within positives.
Recall: The percent of True class which are correctly predicted.
F1_score: The harmonic average of precision and recall.
Exploring performance measures: Common evaluation metrics for classification problems are accuracy, precision, recall, and F1 score. Precision and recall are not good independent performance measures because we can design dummy classifiers to simply boost all of them (Not both at the same time). Therefore, we usually use the F1 score which is a harmonic average of them and cannot be hacked! Among accuracy and F1 score, accuracy is very biased in unbalanced scenarios and can be hacked especially in binary classification problems. So the very robust performance measure under unbalanced scenarios is the F1 score as stated in the literature.

Timing:
The diagram shows that random forest takes an extremely long time (exponentially with the size of the training set) in contrast to the two other classifiers (which appeared linear behavior with increasing training size). The random forest is practically not usable for large training set sizes.

Accuracy:
As can be seen in the diagram the highest accuracy is for the case of no over sampling but the differences are so small ( the minimum accuracy is 99.7%) so no significant difference can be seen within the classifiers as well as related to training set size.

Precision:
Precision always decreases with the increasing training size, the lowest decrement was for XGBOOST and the highest for logistic regression which is a rather considerable decrement compared to the other two classifiers which is as expected for baseline classifier compared to advanced classifiers.

Recall:
The values of recall are very high and the variations in them are too small compared to the values (the minimum recall is about 99.5%). So no major conclusion can be made based on the recall values except that the recall remains constant among the classifiers and train set size.

F1_score:
F1_score always decreases with the increasing training size, the lowest decrement was for XGBOOST and the highest for logistic regression which is a rather considerable decrement compared to the other two classifiers which is as expected for baseline classifier compared to advanced classifiers.
As F1_score is the best performance measure (among the above measurements) for the imbalanced data, we can conclude that the best performance was earned for XGBOOST, slightly better than random forest, with excellent performance equal to 97.5% for totally oversampling (worst) case. Linear regression earned poor performance for rather high training sets due to its model simplicity. For small training set size (majorly undersampling) linear regression earned competitive performance compared to XGBOOST.

Error analysis using confusion matrix: Since misclassifications are clearly viewed in the confusion matrix, we plotted confusion matrices against resampling coefficient for 3 classifiers. The plots are shown below. The number of errors clearly increases with increasing training size, the lowest increment is for XGBOOST (29 false predictions), the second place is for random forest (46 wrong predictions) and the last place for logistic regression (102 wrong predictions). We can conclude that XGBOOST has the lowest error rate.
